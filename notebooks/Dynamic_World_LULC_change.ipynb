{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# System Utilities\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "# Earth Engine & Mapping\n",
    "import ee\n",
    "import geemap\n",
    "\n",
    "# Data Handling & Processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import Layout\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Visualization & Plotting\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set theme for Seaborn visualizations\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Set the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1. Loading the variables for selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Dropdown widget configuration\n",
    "parks = [\n",
    "    'Addo Elephant', 'Agulhas', 'Augrabies Falls', 'Bontebok', 'Camdeboo', 'Garden Route',\n",
    "    'Golden Gate Highlands', 'Graspan', 'Groenkloof', 'Kalahari Gemsbok', 'Karoo', 'Kruger',\n",
    "    'Mapungubwe', 'Marakele', 'Mokala', 'Mountain Zebra', 'Namaqua', 'Richtersveld',\n",
    "    'Table Mountain', 'Tankwa Karoo', 'West Coast'\n",
    "]\n",
    "\n",
    "years = [str(y) for y in range(2016, 2024)]  # Generate year options dynamically\n",
    "\n",
    "# Define dropdowns\n",
    "park_dropdown = widgets.Dropdown(options=parks, value='Addo Elephant', description='Park:')\n",
    "year_start_dropdown = widgets.Dropdown(options=years, value='2016', description='Start Year:')\n",
    "year_end_dropdown = widgets.Dropdown(options=years, value='2023', description='End Year:')\n",
    "\n",
    "# Store selections in a dictionary instead of global variables\n",
    "selection = {\n",
    "    \"Park\": park_dropdown.value,\n",
    "    \"Starting Year\": int(year_start_dropdown.value),\n",
    "    \"Ending Year\": int(year_end_dropdown.value)\n",
    "}\n",
    "\n",
    "# Function to update the dictionary and sync with global variables\n",
    "def update_selection(change, key):\n",
    "    selection[key] = int(change.new) if key != \"Park\" else change.new\n",
    "    \n",
    "    # Ensure Years list updates dynamically\n",
    "    if \"Starting Year\" in selection and \"Ending Year\" in selection:\n",
    "        global Years  # Update global variable\n",
    "        Years = list(range(selection[\"Starting Year\"], selection[\"Ending Year\"] + 1))\n",
    "\n",
    "# Attach event listeners\n",
    "park_dropdown.observe(lambda change: update_selection(change, \"Park\"), names='value')\n",
    "year_start_dropdown.observe(lambda change: update_selection(change, \"Starting Year\"), names='value')\n",
    "year_end_dropdown.observe(lambda change: update_selection(change, \"Ending Year\"), names='value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2. Variable parameters (Please select from the list below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Please choose parameters from the dropdown list below\n",
    "display(park_dropdown, year_start_dropdown, year_end_dropdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.3. Static parameters (Based on selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to set any variables\n",
    "# Calculate the list of years from starting_year to ending_year, inclusive\n",
    "Years = list(range(selection[\"Starting Year\"], selection[\"Ending Year\"] + 1))\n",
    "\n",
    "#Setting the properties for the layer to represent the data on the map and for color coding later on\n",
    "dw_vis = {\"min\": 0, \"max\": 8, \"palette\": [\"#419BDF\", \"#397D49\", \"#88B053\", \"#7A87C6\", \"#E49635\", \"#DFC35A\", \"#C4281B\", \"#A59B8F\", \"#B39FE1\"]}\n",
    "\n",
    "# Define class labels\n",
    "class_labels = ['water', 'trees', 'grass', 'flooded_vegetation', 'crops', 'shrub_and_scrub', 'built', 'bare_soil', 'snow_and_ice']\n",
    "\n",
    "# Get the parent directory (one level up) where config.json is stored\n",
    "config_path = Path(\"..\") / \"config.json\"\n",
    "\n",
    "# Load configuration settings\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Set base path dynamically from config.json\n",
    "base_path = Path(\"..\") / config[\"base_path\"]\n",
    "\n",
    "# Define the different sub-areas\n",
    "# CPA: Catchment Protected Area, VPA: Viewshed Protected Area, PNA: Priority Natural Areas, Parks: Park boundaries itself\n",
    "potential_sub_areas = ['CPA', 'VPA', 'PNA', 'Parks', 'Dissolved']\n",
    "\n",
    "# Initialize a list to store only the sub-areas with available shapefiles\n",
    "sub_areas = []\n",
    "\n",
    "# Check each sub-area for an available shapefile\n",
    "for sub_area in potential_sub_areas:\n",
    "    shapefile_path = base_path / sub_area / f\"{selection['Park']}_{sub_area}.shp\"  # Use Pathlib\n",
    "    if shapefile_path.exists():  # Correct way to check if a Path exists\n",
    "        sub_areas.append(sub_area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Initiate the map (set credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Automatically authenticate and initialize Earth Engine when creating the map\n",
    "try:\n",
    "    ee.Initialize()\n",
    "except Exception:\n",
    "    ee.Authenticate()  # Runs only if authentication is required\n",
    "    ee.Initialize()\n",
    "\n",
    "# Initialize and display the interactive map\n",
    "Map = geemap.Map()\n",
    "Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Loading the data (server-side) according to above set parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1. Loading the data in Earth Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary to store results per sub-area and per year\n",
    "results_per_area_and_year = {}\n",
    "\n",
    "for sub_area in sub_areas:\n",
    "    # Construct the path dynamically\n",
    "    park_sub_shp = base_path / sub_area / f\"{selection['Park']}_{sub_area}.shp\"\n",
    "    \n",
    "    # Convert the shapefile to an Earth Engine feature (if the file exists)\n",
    "    if park_sub_shp.exists():\n",
    "        park_sub = geemap.shp_to_ee(str(park_sub_shp))\n",
    "        geometry = park_sub.geometry()\n",
    "        \n",
    "        # Initialize a dictionary for this sub-area to store dw_class objects per year\n",
    "        dw_classes_per_year = {}\n",
    "\n",
    "        for Year in Years:\n",
    "            start_date = f\"{Year}-01-01\"\n",
    "            end_date = f\"{Year}-12-31\"\n",
    "\n",
    "            # Load the DW dataset for the given year and sub-area\n",
    "            dw_classes = geemap.dynamic_world(\n",
    "                geometry, start_date, end_date, return_type=\"class\", reducer=\"mode\"\n",
    "            )\n",
    "            dw_class = dw_classes.clip(geometry)\n",
    "\n",
    "            # Store the dw_class in the dictionary with the year as the key\n",
    "            dw_classes_per_year[Year] = dw_class\n",
    "\n",
    "        # Store the results for this sub-area\n",
    "        results_per_area_and_year[sub_area] = dw_classes_per_year\n",
    "    else:\n",
    "        print(f\"⚠️ Warning: Shapefile not found for {sub_area} → {park_sub_shp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2. Plotting the data on the map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot the default LULC results\n",
    "def plot_lulc():\n",
    "    \"\"\"\n",
    "    Adds LULC data for the 'Dissolved' sub-area and the latest available year to the map.\n",
    "    Users can modify this function to select different sub-areas or years.\n",
    "    \"\"\"\n",
    "    sub_area = \"Dissolved\"  # Default sub-area\n",
    "    year = max(Years)  # Select the latest available year\n",
    "\n",
    "    if sub_area in results_per_area_and_year and year in results_per_area_and_year[sub_area]:\n",
    "        Map.addLayer(results_per_area_and_year[sub_area][year], dw_vis, f\"LULC {sub_area} {year}\", False)\n",
    "        print(f\"✅ Added LULC layer for {sub_area}, {year} to the map.\")\n",
    "    else:\n",
    "        print(f\"⚠️ No data available for {sub_area} in {year}. Modify the function to select a different sub-area or year.\")\n",
    "\n",
    "# Plot the default example (Dissolved + latest year)\n",
    "plot_lulc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Creating the \"Fisnet\" (avoid API overload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.2.1. Load dissolved buffers to get entire extent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dissolved parks and buffers shapefile (main default)\n",
    "parks_shp = base_path / \"dissolved_all_buffers_FINAL.shp\"\n",
    "parks = geemap.shp_to_ee(str(parks_shp))\n",
    "\n",
    "# 💡 TIP: If the fisnet falls strange over the park, try loading the specific park's dissolved shapefile instead\n",
    "# Uncomment the lines below if needed:\n",
    "#park_dissolved_shp = base_path / \"Dissolved\" / f\"{selection['Park']}_Dissolved.shp\"\n",
    "#park_dissolved = geemap.shp_to_ee(str(park_dissolved_shp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.2.2. Create a \"Fishnet\" from above parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the bounding box for the study area before applying the fishnet grid\n",
    "# Get the bounding box geometry from the dissolved parks dataset\n",
    "bounding_box = parks.geometry().bounds()\n",
    "# 💡 TIP: If the dissolved parks dataset does not look correct, try using the alternative below\n",
    "# bounding_box = park_dissolved.geometry().bounds()  # Uncomment if necessary\n",
    "\n",
    "# Convert the bounding box into a dictionary to extract coordinates\n",
    "bounding_box_info = bounding_box.getInfo()\n",
    "\n",
    "# Extract the coordinates of the bounding box\n",
    "# This accesses the first and only element of the 'coordinates' list, which represents the bounding box polygon.\n",
    "coords = bounding_box_info['coordinates'][0]  \n",
    "\n",
    "# Extract min and max longitude and latitude\n",
    "min_lon, min_lat = coords[0]  # Minimum longitude and latitude\n",
    "max_lon, max_lat = coords[2]  # Maximum longitude and latitude\n",
    "\n",
    "# Create an Earth Engine BBox geometry object using these coordinates\n",
    "region = ee.Geometry.BBox(min_lon, min_lat, max_lon, max_lat)\n",
    "\n",
    "# Generate a fishnet (grid) overlaying the region of interest\n",
    "# h_interval & v_interval define the horizontal & vertical spacing of the grid (in degrees)\n",
    "fishnet = geemap.fishnet(region, h_interval=1.0, v_interval=1.0)\n",
    "# 💡 TIP: h_interval & v_interval works good but can be increased/decreased to find balance between API load and performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.2.3. Pre-calculate the sub-area and fishnet intersections (windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to store window geometries for each sub-area\n",
    "window_geometries_per_sub_area = {}\n",
    "\n",
    "for sub_area in sub_areas:\n",
    "    # Construct the dynamic path for the shapefile\n",
    "    park_sub_shp = base_path / sub_area / f\"{selection['Park']}_{sub_area}.shp\"\n",
    "\n",
    "    if park_sub_shp.exists():\n",
    "        park_sub = geemap.shp_to_ee(str(park_sub_shp))\n",
    "        geometry = park_sub.geometry()\n",
    "\n",
    "        # Calculate intersected features for this sub-area\n",
    "        intersected_features = fishnet.map(\n",
    "            lambda feature: ee.Feature(feature).intersection(geometry, ee.ErrorMargin(1))\n",
    "        )\n",
    "\n",
    "        # Initialize window geometries list\n",
    "        window_geometries = []\n",
    "        try:\n",
    "            feature_list = intersected_features.getInfo()['features']\n",
    "            for feature in feature_list:\n",
    "                geom = ee.Geometry(feature['geometry'])\n",
    "                area = geom.area().getInfo()\n",
    "                if area > 1:  # Only keep areas larger than 1m²\n",
    "                    window_geometries.append(geom)\n",
    "\n",
    "            # Store the window geometries for this sub-area\n",
    "            window_geometries_per_sub_area[sub_area] = window_geometries\n",
    "            print(f\"✅ Processed {sub_area}: {len(window_geometries)} window(s) generated.\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error processing {sub_area}: {e}\")\n",
    "    else:\n",
    "        print(f\"⚠️ Shapefile not found: {park_sub_shp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Calculate the number of pixels per class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### ⚠️ NOTE: This is the most time-consuming part of the script (depending on the size of the park's dissolved area) BUT once all the data is fetched from server side the plotting can begin and is quick. ⚠️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base directory for output CSV files dynamically\n",
    "output_base_path = Path(\"..\") / \"data\" / \"DW_datasets\" / selection[\"Park\"]\n",
    "output_base_path.mkdir(parents=True, exist_ok=True)  # Ensure directory exists\n",
    "\n",
    "# Process LULC data per sub-area per year\n",
    "for sub_area in sub_areas:\n",
    "    # Access pre-calculated window geometries for the current sub-area\n",
    "    window_geometries = window_geometries_per_sub_area[sub_area]\n",
    "    all_years_data = []\n",
    "\n",
    "    # Iterate through each year's data for the current sub-area\n",
    "    for year, dw_class in tqdm(results_per_area_and_year[sub_area].items(), desc=f\"Processing Years for {sub_area}\"):\n",
    "        aggregated_pixel_counts = defaultdict(int)\n",
    "\n",
    "        # Perform parallelized reduction across all windows in GEE\n",
    "        def process_window(window_geometry):\n",
    "            pixel_count_stats = dw_class.reduceRegion(\n",
    "                reducer=ee.Reducer.frequencyHistogram(),\n",
    "                geometry=window_geometry,\n",
    "                scale=10,  # Adjust scale based on dataset resolution\n",
    "                maxPixels=1e10\n",
    "            ).getInfo()\n",
    "\n",
    "            return pixel_count_stats.get('label_mode', {})\n",
    "\n",
    "        # Use list comprehension to apply the function across all windows\n",
    "        pixel_counts_list = [process_window(w) for w in tqdm(window_geometries, desc=f\"Processing Windows for Year {year}\")]\n",
    "\n",
    "        # Aggregate pixel counts across windows\n",
    "        for pixel_counts in pixel_counts_list:\n",
    "            for key, count in pixel_counts.items():\n",
    "                aggregated_pixel_counts[key] += count\n",
    "\n",
    "        # Ensure class labels are correctly mapped\n",
    "        mapped_keys = {str(i): label for i, label in enumerate(class_labels)}\n",
    "        pixel_counts_formatted = {mapped_keys.get(key, key): value for key, value in aggregated_pixel_counts.items()}\n",
    "\n",
    "        all_years_data.append({'Year': year, **pixel_counts_formatted})\n",
    "\n",
    "    # Convert results into a DataFrame\n",
    "    df = pd.DataFrame(all_years_data)\n",
    "    df.set_index('Year', inplace=True)\n",
    "\n",
    "    # Define the output filename dynamically\n",
    "    filename = output_base_path / f\"{selection['Park']}_{sub_area}_LULC_from_{selection['Starting Year']}_to_{selection['Ending Year']}.csv\"\n",
    "\n",
    "    # Save DataFrame to CSV\n",
    "    df.to_csv(filename, index=True)\n",
    "    print(f\"✅ Data saved: {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data collection of LULC CHANGES over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Collecting & calculating the LULC change (server-side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary to store LULC change results per sub-area and per year pair\n",
    "results_per_area_and_year_pairs = {}\n",
    "\n",
    "for sub_area in sub_areas:\n",
    "    # Construct the dynamic path for the shapefile\n",
    "    park_sub_shp = base_path / sub_area / f\"{selection['Park']}_{sub_area}.shp\"\n",
    "\n",
    "    if park_sub_shp.exists():\n",
    "        park_sub = geemap.shp_to_ee(str(park_sub_shp))\n",
    "        geometry = park_sub.geometry()\n",
    "\n",
    "        # Retrieve already calculated LULC classifications per year\n",
    "        dw_classes_per_year = results_per_area_and_year.get(sub_area, {})\n",
    "\n",
    "        # Ensure we have at least two years of data to compute changes\n",
    "        if len(dw_classes_per_year) < 2:\n",
    "            print(f\"⚠️ Not enough data for {sub_area} to compute LULC changes.\")\n",
    "            continue\n",
    "\n",
    "        # Compute LULC changes for each pair of consecutive years within this sub-area\n",
    "        dw_classes_per_year_pairs = {}\n",
    "        for year_index in range(len(Years) - 1):\n",
    "            pre_year = Years[year_index]\n",
    "            post_year = Years[year_index + 1]\n",
    "\n",
    "            # Check if both years exist in the dataset\n",
    "            if pre_year in dw_classes_per_year and post_year in dw_classes_per_year:\n",
    "                image_pre = dw_classes_per_year[pre_year].select('label_mode')\n",
    "                image_post = dw_classes_per_year[post_year].select('label_mode')\n",
    "\n",
    "                # Combine the two images into a single image encoding transitions\n",
    "                combined = image_pre.multiply(10).add(image_post)\n",
    "\n",
    "                # Store the combined LULC change image for this year pair within the sub-area\n",
    "                dw_classes_per_year_pairs[f\"{pre_year}-{post_year}\"] = combined\n",
    "\n",
    "        # Store the LULC change results for this sub-area\n",
    "        results_per_area_and_year_pairs[sub_area] = dw_classes_per_year_pairs\n",
    "        print(f\"✅ Processed LULC changes for {sub_area}: {len(dw_classes_per_year_pairs)} year-pair transitions.\")\n",
    "\n",
    "    else:\n",
    "        print(f\"⚠️ Shapefile not found: {park_sub_shp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Populating a csv with the LULC change data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### ⚠️ NOTE: This is an equally time-consuming part of the script (depending on the size of the park's dissolved area) BUT once all the data is fetched from server side the plotting can begin and is quick. ⚠️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process LULC transitions per sub-area\n",
    "for sub_area in sub_areas:\n",
    "    # Access pre-calculated window geometries for the current sub-area\n",
    "    window_geometries = window_geometries_per_sub_area[sub_area]\n",
    "\n",
    "    yearly_transition_counts = {}\n",
    "\n",
    "    # Iterate through each year pair's data for the current sub-area\n",
    "    for year_pair, dw_class in tqdm(results_per_area_and_year_pairs[sub_area].items(), desc=f\"Processing Year Pairs for {sub_area}\"):\n",
    "        year_pair_transition_counts = defaultdict(int)\n",
    "        pre_year, post_year = year_pair.split('-')\n",
    "\n",
    "        # Define transition label mapping once (avoiding recalculating in loops)\n",
    "        transition_label_map = {str(i * 10 + j): f\"{class_labels[i]}_to_{class_labels[j]}\" \n",
    "                                for i in range(len(class_labels)) for j in range(len(class_labels))}\n",
    "\n",
    "        # Perform parallelized reduction across all windows in GEE\n",
    "        def process_window(window_geometry):\n",
    "            transition_counts = dw_class.reduceRegion(\n",
    "                reducer=ee.Reducer.frequencyHistogram(),\n",
    "                geometry=window_geometry,\n",
    "                scale=10,\n",
    "                maxPixels=1e10\n",
    "            ).getInfo()\n",
    "\n",
    "            return transition_counts.get('label_mode', {})\n",
    "\n",
    "        # Use list comprehension for parallel processing\n",
    "        transition_counts_list = [process_window(w) for w in tqdm(window_geometries, desc=f\"Processing Windows for Year Pair {year_pair}\")]\n",
    "\n",
    "        # Aggregate transition counts across all windows\n",
    "        for transition_counts_dict in transition_counts_list:\n",
    "            for combined_value, count in transition_counts_dict.items():\n",
    "                transition_label = transition_label_map.get(str(combined_value), f\"Unknown_{combined_value}\")\n",
    "                year_pair_transition_counts[transition_label] += count\n",
    "\n",
    "        # Store the aggregated counts for the current year transition\n",
    "        yearly_transition_counts[f\"{pre_year}_to_{post_year}\"] = year_pair_transition_counts\n",
    "\n",
    "    # Convert the dictionary to a DataFrame for CSV export\n",
    "    df_transitions = pd.DataFrame(yearly_transition_counts).fillna(0).reset_index().rename(columns={'index': 'Change'})\n",
    "\n",
    "    # Ensure proper ordering of columns based on transition years\n",
    "    ordered_cols = ['Change'] + sorted(df_transitions.columns[1:], key=lambda x: int(x.split('_to_')[0]) if x.split('_to_')[0].isdigit() else float('inf'))\n",
    "    df_transitions = df_transitions[ordered_cols]\n",
    "\n",
    "    # Define dynamic output filename\n",
    "    csv_file_path = output_base_path / f\"{selection['Park']}_{sub_area}_LULC_change_from_{selection['Starting Year']}_to_{selection['Ending Year']}.csv\"\n",
    "\n",
    "    # Save DataFrame to CSV\n",
    "    df_transitions.to_csv(csv_file_path, index=False)\n",
    "    print(f\"✅ Data saved: {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Plotting the data collected above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Characterize the buffer zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load CSV files safely\n",
    "def load_csv(file_path):\n",
    "    \"\"\"Loads a CSV file into a DataFrame, handling missing files gracefully.\"\"\"\n",
    "    if not file_path.exists():\n",
    "        print(f\"Warning: File not found - {file_path}\")\n",
    "        return None\n",
    "    return pd.read_csv(file_path, index_col=\"Year\")\n",
    "\n",
    "# Function to plot line charts\n",
    "def plot_line_chart(ax, df, valid_labels, linestyle, label_prefix=\"\"):\n",
    "    \"\"\"Plots line charts for the given DataFrame and valid class labels.\"\"\"\n",
    "    for column in valid_labels:\n",
    "        if column in df.columns:\n",
    "            color = palette[class_labels.index(column)]\n",
    "            ax.plot(df.index, df[column], marker=\"o\", linestyle=linestyle, label=f\"{label_prefix}{column}\", color=color)\n",
    "\n",
    "# Function to add pie charts\n",
    "def add_pie_charts(fig, ax, dissolved_df, valid_labels):\n",
    "    \"\"\"Adds pie charts to the plot for each year.\"\"\"\n",
    "    relative_pie_size = 0.15\n",
    "    xticks = ax.get_xticks()\n",
    "    xticks = xticks[(xticks >= dissolved_df.index.min()) & (xticks <= dissolved_df.index.max())]\n",
    "    xticks = xticks[:len(dissolved_df.index)]\n",
    "    \n",
    "    for i, year in enumerate(dissolved_df.index):\n",
    "        pie_x = xticks[i]\n",
    "        trans = ax.transData + fig.transFigure.inverted()\n",
    "        pie_x_fig, _ = trans.transform((pie_x, 0))\n",
    "        pie_rect = [pie_x_fig - relative_pie_size / 2, 0.1, relative_pie_size, relative_pie_size]\n",
    "        \n",
    "        pie_data = dissolved_df.loc[year, valid_labels]\n",
    "        pie_colors = [palette[class_labels.index(col)] for col in pie_data.index]\n",
    "\n",
    "        ax_pie = fig.add_axes(pie_rect, frameon=False)\n",
    "        ax_pie.pie(pie_data, colors=pie_colors, startangle=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define repository-relative paths\n",
    "park_dir = Path(\"..\") / \"data\" / \"DW_datasets\" / selection[\"Park\"]\n",
    "dissolved_file = park_dir / f\"{selection['Park']}_Dissolved_LULC_from_{selection['Starting Year']}_to_{selection['Ending Year']}.csv\"\n",
    "parks_file = park_dir / f\"{selection['Park']}_Parks_LULC_from_{selection['Starting Year']}_to_{selection['Ending Year']}.csv\"\n",
    "\n",
    "# Load CSV data\n",
    "dissolved_df = load_csv(dissolved_file)\n",
    "parks_df = load_csv(parks_file)\n",
    "\n",
    "# Ensure DataFrames are valid before proceeding\n",
    "if dissolved_df is None or parks_df is None:\n",
    "    print(\"Error: One or more required files are missing. Please check the dataset directory.\")\n",
    "else:\n",
    "    # Fill NaN values\n",
    "    dissolved_df = dissolved_df.fillna(0)\n",
    "    parks_df = parks_df.fillna(0)\n",
    "\n",
    "    # Identify valid class labels\n",
    "    valid_class_labels = [label for label in class_labels if label in dissolved_df.columns or label in parks_df.columns]\n",
    "\n",
    "    # Setup figure\n",
    "    fig, ax = plt.subplots(figsize=(15, 10))\n",
    "\n",
    "    # Plot line charts\n",
    "    plot_line_chart(ax, dissolved_df, valid_class_labels, linestyle=\"-\")\n",
    "    plot_line_chart(ax, parks_df, valid_class_labels, linestyle=\"--\", label_prefix=\"Park \")\n",
    "\n",
    "    # Customize plot\n",
    "    ax.set_xlabel(\"Year\")\n",
    "    ax.set_ylabel(\"Pixel count\")\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.set_title(f\"LULC for {selection['Park']}'s park vs. dissolved buffer zone from {selection['Starting Year']} to {selection['Ending Year']}\", fontsize=18)\n",
    "    ax.legend(title=\"Class\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "    ax.set_xticks(dissolved_df.index)\n",
    "    ax.tick_params(axis=\"x\", rotation=45)\n",
    "    ax.grid(True, which=\"both\", linestyle=\"--\")\n",
    "\n",
    "    # Add pie charts\n",
    "    add_pie_charts(fig, ax, dissolved_df, valid_class_labels)\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.subplots_adjust(bottom=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Plotting the normalized yearly difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate yearly differences and normalize them\n",
    "def calculate_normalized_diff(df):\n",
    "    \"\"\"\n",
    "    Calculates the difference between consecutive years and normalizes them\n",
    "    to the maximum absolute value for each column.\n",
    "    \"\"\"\n",
    "    diff_df = df.diff().fillna(0)  # Compute differences, filling NaN with 0\n",
    "    max_values = diff_df.abs().max()  # Maximum absolute values per column\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    max_values[max_values == 0] = 1  # Set max to 1 where values are all zero to prevent NaN\n",
    "    \n",
    "    normalized_diff = diff_df / max_values  # Normalize\n",
    "    return normalized_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute normalized differences\n",
    "dissolved_diff_normalized = calculate_normalized_diff(dissolved_df)\n",
    "parks_diff_normalized = calculate_normalized_diff(parks_df)\n",
    "\n",
    "# Identify available classes present in both datasets\n",
    "available_classes = [label for label in class_labels if label in dissolved_diff_normalized.columns and label in parks_diff_normalized.columns]\n",
    "num_classes = len(available_classes)\n",
    "\n",
    "# Dynamically determine subplot layout\n",
    "ncols = 3\n",
    "nrows = (num_classes + ncols - 1) // ncols  # Round up to ensure all classes fit\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, column in enumerate(available_classes):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Define bar width and x-axis positions\n",
    "    width = 0.35\n",
    "    x = np.arange(len(dissolved_diff_normalized.index))\n",
    "    \n",
    "    # Get color from palette\n",
    "    color = palette[class_labels.index(column)]\n",
    "\n",
    "    # Plot for dissolved buffer zones\n",
    "    ax.bar(x - width/2, dissolved_diff_normalized[column], width, label=f'Dissolved {column}', color=color)\n",
    "    \n",
    "    # Plot for parks with dashed edge and hatch\n",
    "    ax.bar(x + width/2, parks_diff_normalized[column], width, label=f'Park {column}', \n",
    "           color='none', edgecolor=color, linestyle='--', hatch='//')\n",
    "    \n",
    "    # Axis customization\n",
    "    ax.set_title(column)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(dissolved_diff_normalized.index, rotation=45)\n",
    "    ax.set_ylim(-1.1, 1.1)  # Set y-axis range to [-1, 1]\n",
    "    ax.legend()\n",
    "    ax.grid(True, which=\"both\", linestyle=\"--\")\n",
    "\n",
    "# Remove empty subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "# Global title and layout adjustments\n",
    "plt.suptitle(f\"Normalized Yearly Difference in LULC for {selection['Park']}'s Park vs. Dissolved Buffer Zone \"\n",
    "             f\"from {selection['Starting Year']} to {selection['Ending Year']}\", fontsize=18)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Plotting the LULCC on Sankey diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1. Prepare the data in Sankey format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dissolved buffer zones\n",
    "dissolved_change_df = pd.read_csv(fr'C:\\\\Users\\\\grobler\\\\Desktop\\\\Personal\\\\Masters\\\\Data\\\\DW_datasets\\\\{Park}\\\\{Park}_Dissolved_LULC_change_from_{starting_year}_to_{ending_year}.csv', index_col='Change')\n",
    "\n",
    "# Initialize lists to store the source, target, and values for the Sankey diagram\n",
    "sources = []\n",
    "targets = []\n",
    "values = []\n",
    "\n",
    "# Number of class labels\n",
    "num_labels = len(class_labels)\n",
    "\n",
    "# Generate sources, targets, and values from the DataFrame\n",
    "for col in dissolved_change_df.columns:\n",
    "    from_year, to_year = col.split('_to_')\n",
    "    for index, value in dissolved_change_df[col].items():\n",
    "        if value > 0:  # Only create a link if there's a non-zero value\n",
    "            from_class, to_class = index.split('_to_')\n",
    "            source_index = (int(from_year) - starting_year) * num_labels + class_labels.index(from_class)\n",
    "            target_index = (int(to_year) - starting_year) * num_labels + class_labels.index(to_class)\n",
    "            sources.append(source_index)\n",
    "            targets.append(target_index)\n",
    "            values.append(value)\n",
    "\n",
    "# Define node labels and colors (repeated for each year)\n",
    "node_labels = [f'{label}' for year in range(starting_year, ending_year + 1) for label in class_labels]\n",
    "node_colors = palette * (ending_year - starting_year + 1)  # Ensure each node has a color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dissolved_change_data(selection):\n",
    "    \"\"\"\n",
    "    Loads the dissolved LULC change data from the CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - selection (dict): Contains 'Park', 'Starting Year', and 'Ending Year'.\n",
    "\n",
    "    Returns:\n",
    "    - dissolved_change_df (pd.DataFrame): DataFrame containing LULC changes.\n",
    "    \"\"\"\n",
    "    # Define file path relative to the GitHub repository\n",
    "    park_dir = Path(\"..\") / \"data\" / \"DW_datasets\" / selection[\"Park\"]\n",
    "    change_file = park_dir / f\"{selection['Park']}_Dissolved_LULC_change_from_{selection['Starting Year']}_to_{selection['Ending Year']}.csv\"\n",
    "\n",
    "    # Ensure file exists\n",
    "    if not change_file.exists():\n",
    "        raise FileNotFoundError(f\"Error: File not found - {change_file}\")\n",
    "\n",
    "    return pd.read_csv(change_file, index_col=\"Change\")\n",
    "\n",
    "def process_sankey_data(dissolved_change_df, selection, class_labels, palette):\n",
    "    \"\"\"\n",
    "    Processes LULC change data to generate Sankey diagram inputs.\n",
    "\n",
    "    Parameters:\n",
    "    - dissolved_change_df (pd.DataFrame): DataFrame containing LULC changes.\n",
    "    - selection (dict): Contains 'Park', 'Starting Year', and 'Ending Year'.\n",
    "    - class_labels (list): List of LULC class names.\n",
    "    - palette (list): List of colors corresponding to class labels.\n",
    "\n",
    "    Returns:\n",
    "    - sources (list): Source indices for transitions.\n",
    "    - targets (list): Target indices for transitions.\n",
    "    - values (list): Magnitude of transitions.\n",
    "    - node_labels (list): Labels for each node.\n",
    "    - node_colors (list): Colors associated with nodes.\n",
    "    \"\"\"\n",
    "    sources = []\n",
    "    targets = []\n",
    "    values = []\n",
    "    \n",
    "    num_labels = len(class_labels)  # Number of LULC classes\n",
    "\n",
    "    # Generate sources, targets, and values from the DataFrame\n",
    "    for col in dissolved_change_df.columns:\n",
    "        try:\n",
    "            from_year, to_year = map(int, col.split(\"_to_\"))  # Extract years from column name\n",
    "        except ValueError:\n",
    "            print(f\"Skipping column {col}: Invalid format.\")\n",
    "            continue\n",
    "\n",
    "        for index, value in dissolved_change_df[col].items():\n",
    "            if value > 0:  # Only create a link if there's a non-zero value\n",
    "                try:\n",
    "                    from_class, to_class = index.split(\"_to_\")  # Extract LULC classes\n",
    "                except ValueError:\n",
    "                    print(f\"Skipping row {index}: Invalid format.\")\n",
    "                    continue\n",
    "\n",
    "                if from_class in class_labels and to_class in class_labels:\n",
    "                    source_index = (from_year - selection[\"Starting Year\"]) * num_labels + class_labels.index(from_class)\n",
    "                    target_index = (to_year - selection[\"Starting Year\"]) * num_labels + class_labels.index(to_class)\n",
    "\n",
    "                    sources.append(source_index)\n",
    "                    targets.append(target_index)\n",
    "                    values.append(value)\n",
    "\n",
    "    # Define node labels (repeated for each year)\n",
    "    node_labels = [f\"{label}\" for year in range(selection[\"Starting Year\"], selection[\"Ending Year\"] + 1) for label in class_labels]\n",
    "    node_colors = palette * (selection[\"Ending Year\"] - selection[\"Starting Year\"] + 1)  # Repeat colors for each year\n",
    "\n",
    "    return sources, targets, values, node_labels, node_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "dissolved_change_df = load_dissolved_change_data(selection)\n",
    "\n",
    "# Process the data for the Sankey diagram\n",
    "sources, targets, values, node_labels, node_colors = process_sankey_data(dissolved_change_df, selection, class_labels, palette)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2. Plot the Sankey-diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sankey_diagram(sources, targets, values, node_labels, node_colors, selection):\n",
    "    \"\"\"\n",
    "    Generates a Sankey diagram for visualizing LULC change over time.\n",
    "\n",
    "    Parameters:\n",
    "    - sources (list): List of source node indices.\n",
    "    - targets (list): List of target node indices.\n",
    "    - values (list): List of flow values.\n",
    "    - node_labels (list): Labels for each node.\n",
    "    - node_colors (list): Colors for each node.\n",
    "    - selection (dict): Contains 'Park', 'Starting Year', and 'Ending Year'.\n",
    "    \"\"\"\n",
    "    if not (len(sources) == len(targets) == len(values)):\n",
    "        raise ValueError(\"Mismatch in the length of sources, targets, and values.\")\n",
    "\n",
    "    if len(node_labels) != len(node_colors):\n",
    "        raise ValueError(\"Mismatch between the number of node labels and node colors.\")\n",
    "\n",
    "    # Create Sankey Diagram\n",
    "    fig = go.Figure(data=[go.Sankey(\n",
    "        node=dict(\n",
    "            pad=15,\n",
    "            thickness=20,\n",
    "            line=dict(color=\"black\", width=0.5),\n",
    "            label=node_labels,\n",
    "            color=node_colors\n",
    "        ),\n",
    "        link=dict(\n",
    "            source=sources,\n",
    "            target=targets,\n",
    "            value=values\n",
    "        )\n",
    "    )])\n",
    "\n",
    "    # Calculate proportional positions for year annotations along the x-axis\n",
    "    starting_year = selection[\"Starting Year\"]\n",
    "    ending_year = selection[\"Ending Year\"]\n",
    "    \n",
    "    year_positions = {\n",
    "        year: (year - starting_year) / (ending_year - starting_year) \n",
    "        for year in range(starting_year, ending_year + 1)\n",
    "    }\n",
    "\n",
    "    # Update layout with title and year annotations\n",
    "    fig.update_layout(\n",
    "        title=dict(\n",
    "            text=f\"LULC Change for {selection['Park']}'s Dissolved Buffer Zone from {starting_year} to {ending_year}\",\n",
    "            font=dict(size=20)\n",
    "        ),\n",
    "        font=dict(size=10),\n",
    "        annotations=[\n",
    "            dict(\n",
    "                showarrow=False,\n",
    "                text=str(year),\n",
    "                xref=\"paper\",\n",
    "                yref=\"paper\",\n",
    "                x=year_positions[year],\n",
    "                y=-0.1,\n",
    "                align=\"center\",\n",
    "                font=dict(size=10)\n",
    "            ) for year in range(starting_year, ending_year + 1)\n",
    "        ],\n",
    "        height=600\n",
    "    )\n",
    "\n",
    "    # Show the Sankey diagram\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Sankey diagram\n",
    "plot_sankey_diagram(sources, targets, values, node_labels, node_colors, selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Calculate the LULCC intensity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Choose sub-area for further investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure sub_areas is defined\n",
    "if 'sub_areas' not in globals():\n",
    "    raise ValueError(\"Error: 'sub_areas' list is not defined. Please ensure it's available before running this cell.\")\n",
    "\n",
    "# Define the Dropdown Widget\n",
    "sub_area_dropdown = widgets.Dropdown(\n",
    "    options=sub_areas,\n",
    "    value=sub_areas[0] if sub_areas else 'Dissolved',  # Default to first option if available\n",
    "    description='Buffer sub-area for further investigation:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# Define an Update Function\n",
    "def inv_sub_area_change(change):\n",
    "    \"\"\"\n",
    "    Updates the selected buffer sub-area when the dropdown value changes.\n",
    "\n",
    "    Parameters:\n",
    "    - change (dict): Contains old and new values for the widget.\n",
    "    \"\"\"\n",
    "    print(f\"Buffer sub-area to investigate changed to: {change['new']}\")\n",
    "\n",
    "# Attach the Update Function to the Dropdown\n",
    "sub_area_dropdown.observe(inv_sub_area_change, names='value')\n",
    "\n",
    "# Print guidance for users\n",
    "print(\n",
    "    \"💡 TIP: Start with 'Dissolved' and then explore other options as needed.\\n\\n\"\n",
    "    \"Abbreviations:\\n\"\n",
    "    \"- CPA: Catchment Protected Area\\n\"\n",
    "    \"- VPA: Viewshed Protected Area\\n\"\n",
    "    \"- PNA: Priority Natural Areas\\n\"\n",
    "    \"- Parks: Park boundaries themselves\"\n",
    ")\n",
    "\n",
    "# Display the dropdown in a Jupyter notebook\n",
    "display(sub_area_dropdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Prepare data in \"cross-tabulation matrix\" format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sub_area_data(selection, sub_area):\n",
    "    \"\"\"\n",
    "    Loads LULC transition data for a selected sub-area.\n",
    "\n",
    "    Parameters:\n",
    "    - selection (dict): Contains 'Park', 'Starting Year', and 'Ending Year'.\n",
    "    - sub_area (str): Selected buffer sub-area.\n",
    "\n",
    "    Returns:\n",
    "    - df (pd.DataFrame): DataFrame containing LULC transitions.\n",
    "    \"\"\"\n",
    "    # Define the path relative to the repository\n",
    "    park_dir = Path(\"..\") / \"data\" / \"DW_datasets\" / selection[\"Park\"]\n",
    "    file_path = park_dir / f\"{selection['Park']}_{sub_area}_LULC_change_from_{selection['Starting Year']}_to_{selection['Ending Year']}.csv\"\n",
    "\n",
    "    # Check if file exists before reading\n",
    "    if not file_path.exists():\n",
    "        raise FileNotFoundError(f\"Error: File not found - {file_path}\")\n",
    "\n",
    "    # Load CSV into DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Ensure the 'Change' column exists for processing\n",
    "    if \"Change\" not in df.columns:\n",
    "        raise KeyError(f\"Error: Column 'Change' not found in {file_path}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def compute_transition_matrices(df):\n",
    "    \"\"\"\n",
    "    Computes yearly LULC transition matrices.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): DataFrame containing LULC transitions.\n",
    "\n",
    "    Returns:\n",
    "    - transition_matrices (dict): Dictionary of DataFrames, each representing a transition matrix for a year.\n",
    "    \"\"\"\n",
    "    # Ensure 'Change' column is split into 'from' and 'to'\n",
    "    if \"Change\" in df.columns:\n",
    "        df[['from', 'to']] = df['Change'].str.split('_to_', expand=True)\n",
    "    else:\n",
    "        raise KeyError(\"Error: 'Change' column is missing in the dataset.\")\n",
    "\n",
    "    # Initialize a dictionary to store transition matrices\n",
    "    transition_matrices = {}\n",
    "\n",
    "    # Identify relevant year columns (excluding 'Change', 'from', 'to')\n",
    "    year_columns = [col for col in df.columns if col not in [\"Change\", \"from\", \"to\"]]\n",
    "\n",
    "    for year in year_columns:\n",
    "        # Pivot to create transition matrix for the current year\n",
    "        matrix = df.pivot(index='from', columns='to', values=year).fillna(0)\n",
    "\n",
    "        # Ensure all LULC classes exist in both 'from' and 'to' categories\n",
    "        all_classes = sorted(set(matrix.index) | set(matrix.columns))\n",
    "        matrix = matrix.reindex(index=all_classes, columns=all_classes, fill_value=0)\n",
    "\n",
    "        # Calculate summary statistics\n",
    "        matrix = matrix.T  # Transpose for calculation\n",
    "        matrix['Final total'] = matrix.sum(axis=1)\n",
    "\n",
    "        # SAFE diagonal extraction (Gross Gain)\n",
    "        matrix['Gross gain'] = matrix['Final total'] - [\n",
    "            matrix.at[state, state] if state in matrix.columns and state in matrix.index else 0 for state in matrix.index\n",
    "        ]\n",
    "\n",
    "        # Flip back to original orientation\n",
    "        matrix = matrix.T\n",
    "        matrix['Initial total'] = matrix.sum(axis=1)\n",
    "\n",
    "        # SAFE diagonal extraction (Gross Loss)\n",
    "        matrix['Gross loss'] = matrix['Initial total'] - [\n",
    "            matrix.at[state, state] if state in matrix.columns and state in matrix.index else 0 for state in matrix.index\n",
    "        ]\n",
    "\n",
    "        # Store the computed transition matrix\n",
    "        transition_matrices[year] = matrix\n",
    "\n",
    "    return transition_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset for the selected sub-area\n",
    "df = load_sub_area_data(selection, sub_area_dropdown.value)\n",
    "\n",
    "# Compute transition matrices for all years\n",
    "transition_matrices = compute_transition_matrices(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Calcualte the \"Time Intensity\" of LULCC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.1. \"Time Intensity\" calculation (St & U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to hold the calculated values\n",
    "time_intervals = []\n",
    "annual_rates_of_change = []\n",
    "\n",
    "# Loop through each year interval\n",
    "for year_interval, matrix in transition_matrices.items():\n",
    "    try:\n",
    "        # Ensure row labels are treated correctly\n",
    "        if 'Final total' not in matrix.index or 'Gross gain' not in matrix.index:\n",
    "            print(f\"⚠️ Warning: 'Final total' or 'Gross gain' is missing in {year_interval}, skipping.\")\n",
    "            continue  # Skip this year if required data is missing\n",
    "\n",
    "        # Retrieve the total area of change (sum of 'Gross gain' row)\n",
    "        total_area_of_change = matrix.loc['Gross gain'].sum()\n",
    "        # Retrieve the total area of the study region (sum of 'Final total' row)\n",
    "        total_area_of_study_region = matrix.loc['Final total'].sum()\n",
    "\n",
    "        # Assume a duration of the interval in years of 1 for simplicity\n",
    "        duration_of_interval = 1\n",
    "        # Calculate the annual rate of change (time intensity)\n",
    "        time_intensity = (total_area_of_change / total_area_of_study_region) / duration_of_interval * 100\n",
    "\n",
    "        # Store the results\n",
    "        time_intervals.append(year_interval)\n",
    "        annual_rates_of_change.append(time_intensity)\n",
    "\n",
    "    except KeyError as e:\n",
    "        print(f\"⚠️ Warning: Unexpected missing data in transition matrix for {year_interval}: {e}\")\n",
    "        continue  # Skip problematic years\n",
    "\n",
    "# Calculate the uniform intensity value across all intervals\n",
    "try:\n",
    "    # Use only matrices where 'Final total' is present\n",
    "    valid_matrices = [matrix for matrix in transition_matrices.values() if 'Final total' in matrix.index]\n",
    "\n",
    "    if not valid_matrices:\n",
    "        raise ValueError(\"No valid transition matrices found with 'Final total' row.\")\n",
    "\n",
    "    # Sum the 'Gross gain' row across all years\n",
    "    total_change_over_all_intervals = sum(matrix.loc['Gross gain'].sum() for matrix in valid_matrices)\n",
    "    # Use the 'Final total' sum from the first valid matrix\n",
    "    total_study_area = valid_matrices[0].loc['Final total'].sum()\n",
    "    total_time = len(time_intervals)\n",
    "    uniform_intensity = (total_change_over_all_intervals / total_study_area) / total_time * 100\n",
    "\n",
    "except (KeyError, ValueError) as e:\n",
    "    print(f\"⚠️ Warning: Error while calculating uniform intensity: {e}\")\n",
    "    uniform_intensity = None  # Set a default value to avoid errors\n",
    "\n",
    "# Initialize significant_intervals to an empty list\n",
    "significant_intervals = []\n",
    "\n",
    "# Identify time intervals where the annual rate of change is greater than uniform intensity\n",
    "if uniform_intensity is not None:\n",
    "    significant_intervals = [year for year, rate in zip(time_intervals, annual_rates_of_change) if rate > uniform_intensity]\n",
    "\n",
    "print(\"📊 Significant Intervals:\", significant_intervals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2. \"Time Intensity\" plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_time_intensity_analysis(time_intervals, annual_rates_of_change, uniform_intensity, selection, sub_area):\n",
    "    \"\"\"\n",
    "    Plots the annual rates of LULCC change over time and compares them with uniform intensity.\n",
    "\n",
    "    Parameters:\n",
    "    - time_intervals (list): List of time periods (e.g., '2016_to_2017').\n",
    "    - annual_rates_of_change (list): Corresponding annual rates of change (percent values).\n",
    "    - uniform_intensity (float): The uniform intensity threshold for comparison.\n",
    "    - selection (dict): Contains 'Park' name for title.\n",
    "    - sub_area (str): Selected sub-area for investigation.\n",
    "\n",
    "    Returns:\n",
    "    - Displays a horizontal bar plot of the time intensity analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure valid data before plotting\n",
    "    if not time_intervals or not annual_rates_of_change or uniform_intensity is None:\n",
    "        print(\"⚠️ Error: Missing required data for plotting. Ensure time_intervals, annual_rates_of_change, and uniform_intensity are available.\")\n",
    "        return\n",
    "\n",
    "    # Create the figure\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    # Plot annual rates of change as horizontal bars\n",
    "    plt.barh(time_intervals, annual_rates_of_change, color='gray', edgecolor='black', label=\"Annual Rate of Change\")\n",
    "\n",
    "    # Plot the uniform intensity threshold as a red dashed line\n",
    "    plt.axvline(x=uniform_intensity, color='red', linestyle='--', label=f'Uniform Intensity: {uniform_intensity:.2f}%')\n",
    "\n",
    "    # Add \"Slow\" and \"Fast\" labels relative to the uniform intensity line\n",
    "    y_position = max(len(time_intervals) - 1, 1)  # Ensure labels are within plot range\n",
    "    plt.text(uniform_intensity - 0.08, y_position, 'Slow',\n",
    "             verticalalignment='center', horizontalalignment='right', color='red', fontsize=12)\n",
    "    plt.text(uniform_intensity + 0.08, y_position, 'Fast',\n",
    "             verticalalignment='center', horizontalalignment='left', color='red', fontsize=12)\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel('Annual Change Area (percent of map)', fontsize=12)\n",
    "    plt.ylabel('Time Interval', fontsize=12)\n",
    "    plt.title(f\"LULCC Time Intensity Analysis for {selection['Park']}'s {sub_area} Buffer Zone\", fontsize=15)\n",
    "    plt.legend()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the time intensity analysis for the selected sub-area\n",
    "plot_time_intensity_analysis(time_intervals, annual_rates_of_change, uniform_intensity, selection, sub_area_dropdown.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Calcualte the \"Category Intensity\" of LULCC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.1. \"Category Intensity\" calculation (Gtj & Lti)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### ⚠️ Note: this version of the code is developed to only analyse those years with St > U identified above. ⚠️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_category_intensities(significant_intervals, transition_matrices, duration_of_interval=1):\n",
    "    \"\"\"\n",
    "    Computes category gain and loss intensities for each significant time interval.\n",
    "\n",
    "    Parameters:\n",
    "    - significant_intervals (list): List of years with significant LULCC changes.\n",
    "    - transition_matrices (dict): Dictionary containing transition matrices for each year.\n",
    "    - duration_of_interval (int): Number of years in the interval (default=1).\n",
    "\n",
    "    Returns:\n",
    "    - category_intensities (dict): Dictionary with per-category loss and gain intensities.\n",
    "    \"\"\"\n",
    "    # Dictionary to store category intensities per year\n",
    "    category_intensities = {}\n",
    "\n",
    "    # Loop through each significant year\n",
    "    for year in significant_intervals:\n",
    "        # Retrieve the transition matrix for the current year\n",
    "        matrix = transition_matrices[year]\n",
    "\n",
    "        # Initialize dictionaries for loss and gain intensities\n",
    "        loss_intensities = {}\n",
    "        gain_intensities = {}\n",
    "\n",
    "        # Loop through the categories in the matrix (excluding summary rows)\n",
    "        for category in matrix.index[:-2]:  # Excluding 'Final total' and 'Gross gain'\n",
    "            try:\n",
    "                # Retrieve necessary values from the matrix\n",
    "                initial_total = matrix.loc[category, 'Initial total']\n",
    "                final_total = matrix.T.loc[category, 'Final total']\n",
    "                gross_loss = matrix.loc[category, 'Gross loss']\n",
    "                gross_gain = matrix.T.loc[category, 'Gross gain']\n",
    "\n",
    "                # Calculate loss intensity (only if `initial_total` > 0)\n",
    "                if initial_total > 0:\n",
    "                    loss_intensity = (gross_loss / duration_of_interval) / initial_total * 100\n",
    "                    loss_intensities[category] = loss_intensity\n",
    "\n",
    "                # Calculate gain intensity (only if `final_total` > 0)\n",
    "                if final_total > 0:\n",
    "                    gain_intensity = (gross_gain / duration_of_interval) / final_total * 100\n",
    "                    gain_intensities[category] = gain_intensity\n",
    "\n",
    "            except KeyError as e:\n",
    "                print(f\"⚠️ Warning: Missing data for category '{category}' in {year}: {e}\")\n",
    "                continue  # Skip problematic categories\n",
    "\n",
    "        # Store computed intensities for the current year\n",
    "        category_intensities[year] = {\n",
    "            'loss_intensities': loss_intensities,\n",
    "            'gain_intensities': gain_intensities\n",
    "        }\n",
    "\n",
    "    return category_intensities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the method for calculating category intensities\n",
    "category_intensities = calculate_category_intensities(significant_intervals, transition_matrices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.2. \"Category Intensity\" plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_category_intensity_analysis(significant_intervals, category_intensities, time_intervals, annual_rates_of_change, selection, sub_area, total_categories=9):\n",
    "    \"\"\"\n",
    "    Plots the LULCC category intensity analysis for significant intervals.\n",
    "\n",
    "    Parameters:\n",
    "    - significant_intervals (list): List of significant years.\n",
    "    - category_intensities (dict): Dictionary containing per-category gain and loss intensities.\n",
    "    - time_intervals (list): All time intervals.\n",
    "    - annual_rates_of_change (list): Corresponding annual rates of change.\n",
    "    - selection (dict): Contains 'Park' name for title.\n",
    "    - sub_area (str): Selected sub-area for investigation.\n",
    "    - total_categories (int): Total expected LULC categories in the dataset (default=9).\n",
    "\n",
    "    Returns:\n",
    "    - Displays category intensity bar plots for each significant interval.\n",
    "    \"\"\"\n",
    "\n",
    "    for interval in significant_intervals:\n",
    "        # Ensure the interval exists in category_intensities\n",
    "        if interval not in category_intensities:\n",
    "            print(f\"⚠️ Warning: No data found for interval {interval}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Get intensity data for the current interval\n",
    "        data = category_intensities[interval]\n",
    "\n",
    "        # Extract categories and fill missing ones with 0s\n",
    "        categories = list(data['loss_intensities'].keys())  # Available categories\n",
    "        loss_intensities = [data['loss_intensities'].get(cat, 0) for cat in categories]\n",
    "        gain_intensities = [data['gain_intensities'].get(cat, 0) for cat in categories]\n",
    "\n",
    "        # Padding missing categories with zeros\n",
    "        empty_slots = total_categories - len(categories)\n",
    "        loss_intensities += [0] * empty_slots\n",
    "        gain_intensities += [0] * empty_slots\n",
    "        categories += [''] * empty_slots  # Empty strings for missing category labels\n",
    "\n",
    "        # Find the annual rate of change for this interval\n",
    "        index = time_intervals.index(interval)\n",
    "        specific_uniform_intensity = annual_rates_of_change[index]\n",
    "\n",
    "        # Position of bars on the y-axis\n",
    "        y_pos = np.arange(total_categories)\n",
    "\n",
    "        # Create the figure\n",
    "        plt.figure(figsize=(10, 5))\n",
    "\n",
    "        # Horizontal bars for losses\n",
    "        plt.barh(y_pos, loss_intensities, color='tomato', edgecolor='black', height=0.4, label='Loss Intensity')\n",
    "\n",
    "        # Horizontal bars for gains (slightly offset on the y-axis)\n",
    "        plt.barh(y_pos + 0.4, gain_intensities, color='mediumseagreen', edgecolor='black', height=0.4, label='Gain Intensity')\n",
    "\n",
    "        # Draw a dashed line for uniform intensity\n",
    "        plt.axvline(x=specific_uniform_intensity, color='black', linestyle='--', label=f'Uniform Intensity {specific_uniform_intensity:.2f}%')\n",
    "\n",
    "        # Add 'Dormant' and 'Active' labels near the uniform intensity line\n",
    "        plt.text(specific_uniform_intensity - 0.25, total_categories - 0.2, 'Dormant',\n",
    "                 verticalalignment='center', horizontalalignment='right', color='black', fontsize=10)\n",
    "        plt.text(specific_uniform_intensity + 0.5, total_categories - 0.2, 'Active',\n",
    "                 verticalalignment='center', horizontalalignment='left', color='black', fontsize=10)\n",
    "\n",
    "        # Labels and title\n",
    "        plt.xlabel('Annual Change Intensity (percent of category)', fontsize=12)\n",
    "        plt.title(f\"LULCC Category Intensity Analysis for {selection['Park']}'s {sub_area} Buffer Zone ({interval})\", fontsize=13)\n",
    "        plt.yticks(y_pos + 0.2, categories)  # Center y-ticks for categories\n",
    "\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the category intensity analysis results\n",
    "plot_category_intensity_analysis(significant_intervals, category_intensities, time_intervals, annual_rates_of_change, selection, sub_area_dropdown.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Calcualte the \"Transition Intensity\" of LULCC (Qtmj, Vtm, Rtin and Wtn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.1. \"Transition Intensity\" calculation for Qtmj, Vtm -> Loss, transition from target category to all other categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_category_transition_intensities(significant_intervals, transition_matrices, duration_of_interval=1):\n",
    "    \"\"\"\n",
    "    Computes category transition intensities for each significant time interval.\n",
    "\n",
    "    Parameters:\n",
    "    - significant_intervals (list): List of years with significant LULCC changes.\n",
    "    - transition_matrices (dict): Dictionary containing transition matrices for each year.\n",
    "    - duration_of_interval (int): Number of years in the interval (default=1).\n",
    "\n",
    "    Returns:\n",
    "    - category_transitions (dict): Dictionary with transition intensities for each category.\n",
    "    \"\"\"\n",
    "    category_transitions = {}\n",
    "\n",
    "    for year in significant_intervals:\n",
    "        matrix = transition_matrices.get(year)\n",
    "        if matrix is None:\n",
    "            print(f\"⚠️ Warning: No transition matrix found for {year}, skipping.\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Extract 'Final total' row, excluding the last two summary rows ('Final total' & 'Gross gain')\n",
    "            final_totals = matrix.loc['Final total'].iloc[:-2]\n",
    "            total_area = final_totals.sum()\n",
    "\n",
    "            # Initialize a DataFrame to store transition intensities for the current year\n",
    "            transition_intensities = pd.DataFrame(0, \n",
    "                                                  index=matrix.index[:-2], \n",
    "                                                  columns=matrix.columns[:-2].append(pd.Index(['Uniform_Intensity'])))\n",
    "\n",
    "            for category_from in matrix.index[:-2]:  # Exclude 'Final total' and 'Gross gain' rows\n",
    "                gross_loss = matrix.loc[category_from, 'Gross loss']\n",
    "                area_not_m = total_area - matrix.loc['Final total', category_from]\n",
    "\n",
    "                if area_not_m > 0:  # Avoid division by zero\n",
    "                    uniform_intensity = (gross_loss / duration_of_interval) / area_not_m * 100\n",
    "                    transition_intensities.loc[category_from, 'Uniform_Intensity'] = uniform_intensity\n",
    "\n",
    "                for category_to in matrix.columns[:-2]:  # Exclude 'Initial total' and 'Gross loss' columns\n",
    "                    if category_from != category_to:  # Ensure transitions between different categories\n",
    "                        transition_area = matrix.loc[category_from, category_to]\n",
    "                        final_total_category_to = matrix.loc['Final total', category_to]\n",
    "\n",
    "                        if final_total_category_to > 0:  # Avoid division by zero\n",
    "                            transition_intensity = (transition_area / duration_of_interval) / final_total_category_to * 100\n",
    "                            transition_intensities.at[category_from, category_to] = transition_intensity\n",
    "\n",
    "            # Store computed transition intensities for the current year\n",
    "            category_transitions[year] = transition_intensities\n",
    "\n",
    "        except KeyError as e:\n",
    "            print(f\"⚠️ Warning: Missing data in transition matrix for {year}: {e}\")\n",
    "            continue  # Skip problematic years\n",
    "\n",
    "    return category_transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the method for calculating category transition intensities (from target category)\n",
    "category_transitions = calculate_category_transition_intensities(significant_intervals, transition_matrices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.2. \"Transition Intensity\" plots for Qtmj, Vtm -> Loss, transition from target category to all other categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_transition_heatmap(significant_intervals, category_transitions, selection, sub_area):\n",
    "    \"\"\"\n",
    "    Plots LULC transition intensity heatmaps for significant intervals.\n",
    "\n",
    "    Parameters:\n",
    "    - significant_intervals (list): List of significant years.\n",
    "    - category_transitions (dict): Dictionary containing transition intensities.\n",
    "    - selection (dict): Contains 'Park' name for title.\n",
    "    - sub_area (str): Selected sub-area for investigation.\n",
    "\n",
    "    Returns:\n",
    "    - Displays a heatmap for each significant interval.\n",
    "    \"\"\"\n",
    "    for year in significant_intervals:\n",
    "        # Ensure the year exists in category_transitions\n",
    "        if year not in category_transitions:\n",
    "            print(f\"⚠️ Warning: No data found for interval {year}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        matrix_to_plot = category_transitions[year].round(0)  # Round values for cleaner annotations\n",
    "\n",
    "        # Create a mask to hide diagonal values\n",
    "        mask = np.zeros_like(matrix_to_plot, dtype=bool)\n",
    "        np.fill_diagonal(mask, True)\n",
    "\n",
    "        # Ensure a copy of colormap before modifying\n",
    "        cmap = mpl.colormaps.get_cmap('Greens').copy()\n",
    "        cmap.set_bad(\"white\")  # Set masked elements to white\n",
    "\n",
    "        # Create the heatmap figure\n",
    "        plt.figure(figsize=(12, 9))\n",
    "        ax = sns.heatmap(matrix_to_plot, annot=True, fmt=\".0f\", cmap=cmap, linewidths=0.5, mask=mask)\n",
    "\n",
    "        # Add vertical lines to separate categories\n",
    "        for i in range(matrix_to_plot.shape[1] + 1):\n",
    "            plt.axvline(x=i, color='black', linestyle='-', linewidth=1)\n",
    "\n",
    "        # Labels and title\n",
    "        plt.title(f\"LULC Transition Intensity Analysis for {selection['Park']}'s {sub_area} Buffer Zone ({year})\", fontsize=14)\n",
    "        plt.xlabel('To Category & Uniform Intensity (percent of category)', fontsize=12)\n",
    "        plt.xticks(rotation=45, ha=\"right\")\n",
    "        plt.ylabel('From Category (percent of category)', fontsize=12)\n",
    "\n",
    "        # Show the plot\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the category transition intensity analysis results (transition from target category)\n",
    "plot_transition_heatmap(significant_intervals, category_transitions, selection, sub_area_dropdown.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.3. \"Transition Intensity\" calculation for Rtin, Wtn -> Gain, transition to target category from all other categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_category_transition_intensities_v2(significant_intervals, transition_matrices, duration_of_interval=1):\n",
    "    \"\"\"\n",
    "    Computes category transition intensities for each significant time interval.\n",
    "\n",
    "    Parameters:\n",
    "    - significant_intervals (list): List of years with significant LULCC changes.\n",
    "    - transition_matrices (dict): Dictionary containing transition matrices for each year.\n",
    "    - duration_of_interval (int): Number of years in the interval (default=1).\n",
    "\n",
    "    Returns:\n",
    "    - category_transitions (dict): Dictionary with transition intensities for each category.\n",
    "    \"\"\"\n",
    "    category_transitions = {}\n",
    "\n",
    "    for year in significant_intervals:\n",
    "        matrix = transition_matrices.get(year)\n",
    "        if matrix is None:\n",
    "            print(f\"⚠️ Warning: No transition matrix found for {year}, skipping.\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Extract 'Initial total' column, excluding last two summary rows ('Final total' & 'Gross gain')\n",
    "            initial_totals = matrix['Initial total'].iloc[:-2]\n",
    "            total_area = initial_totals.sum()\n",
    "\n",
    "            # Initialize DataFrame for transition intensities\n",
    "            transition_intensities = pd.DataFrame(0, \n",
    "                                                  index=matrix.index[:-2], \n",
    "                                                  columns=matrix.columns[:-2].append(pd.Index(['Uniform_Intensity'])))\n",
    "\n",
    "            for category_from in matrix.index[:-2]:  # Exclude 'Final total' and 'Gross gain' rows\n",
    "                gross_gain = matrix.loc['Gross gain', category_from]\n",
    "                area_not_n = total_area - matrix.loc[category_from, 'Initial total']\n",
    "\n",
    "                if area_not_n > 0:  # Avoid division by zero\n",
    "                    uniform_intensity = (gross_gain / duration_of_interval) / area_not_n * 100\n",
    "                    transition_intensities.loc[category_from, 'Uniform_Intensity'] = uniform_intensity\n",
    "\n",
    "                for category_to in matrix.columns[:-2]:  # Exclude 'Initial total' and 'Gross loss'\n",
    "                    if category_from != category_to:  # Ensure transitions between different categories\n",
    "                        transition_area = matrix.loc[category_from, category_to]\n",
    "                        initial_total_category_from = matrix.loc[category_from, 'Initial total']\n",
    "\n",
    "                        if initial_total_category_from > 0:  # Avoid division by zero\n",
    "                            transition_intensity = (transition_area / duration_of_interval) / initial_total_category_from * 100\n",
    "                            transition_intensities.at[category_from, category_to] = transition_intensity\n",
    "\n",
    "            # Extract the 'Uniform_Intensity' column as a separate Series\n",
    "            uniform_intensity_row = transition_intensities['Uniform_Intensity'].copy()\n",
    "\n",
    "            # Remove the 'Uniform_Intensity' column\n",
    "            transition_intensities.drop('Uniform_Intensity', axis=1, inplace=True)\n",
    "\n",
    "            # Append the 'Uniform_Intensity' Series as a new row\n",
    "            transition_intensities.loc['Uniform_Intensity'] = uniform_intensity_row\n",
    "\n",
    "            # Store transition intensities matrix in dictionary\n",
    "            category_transitions[year] = transition_intensities\n",
    "\n",
    "        except KeyError as e:\n",
    "            print(f\"⚠️ Warning: Missing data in transition matrix for {year}: {e}\")\n",
    "            continue  # Skip problematic years\n",
    "\n",
    "    return category_transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the method for calculating category transition intensities (to target category)\n",
    "category_transitions = calculate_category_transition_intensities_v2(significant_intervals, transition_matrices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.4. \"Transition Intensity\" plots for Rtin, Wtn -> Gain, transition to target category from all other categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_transition_heatmap_v2(significant_intervals, category_transitions, selection, sub_area):\n",
    "    \"\"\"\n",
    "    Plots LULC transition intensity heatmaps for significant intervals.\n",
    "\n",
    "    Parameters:\n",
    "    - significant_intervals (list): List of significant years.\n",
    "    - category_transitions (dict): Dictionary containing transition intensities.\n",
    "    - selection (dict): Contains 'Park' name for title.\n",
    "    - sub_area (str): Selected sub-area for investigation.\n",
    "\n",
    "    Returns:\n",
    "    - Displays a heatmap for each significant interval.\n",
    "    \"\"\"\n",
    "    for year in significant_intervals:\n",
    "        # Ensure the year exists in category_transitions\n",
    "        if year not in category_transitions:\n",
    "            print(f\"⚠️ Warning: No data found for interval {year}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        matrix_to_plot = category_transitions[year].round(0)  # Round values for cleaner annotations\n",
    "\n",
    "        # Create a mask to hide diagonal values\n",
    "        mask = np.zeros_like(matrix_to_plot, dtype=bool)\n",
    "        np.fill_diagonal(mask, True)\n",
    "\n",
    "        # Ensure a copy of colormap before modifying\n",
    "        cmap = mpl.colormaps.get_cmap('Reds').copy()\n",
    "        cmap.set_bad(\"white\")  # Set masked elements to white\n",
    "\n",
    "        # Create the heatmap figure\n",
    "        plt.figure(figsize=(12, 9))\n",
    "        ax = sns.heatmap(matrix_to_plot, annot=True, fmt=\".0f\", cmap=cmap, linewidths=0.5, mask=mask)\n",
    "\n",
    "        # Add horizontal lines to separate categories\n",
    "        for i in range(matrix_to_plot.shape[0] + 1):\n",
    "            plt.axhline(y=i, color='black', linestyle='-', linewidth=1)\n",
    "\n",
    "        # Labels and title\n",
    "        plt.title(f\"LULC Transition Intensity Analysis for {selection['Park']}'s {sub_area} Buffer Zone ({year})\", fontsize=14)\n",
    "        plt.xlabel('To Category (percent of category)', fontsize=12)\n",
    "        plt.xticks(rotation=45, ha=\"right\")\n",
    "        plt.ylabel('From Category & Uniform Intensity (percent of category)', fontsize=12)\n",
    "\n",
    "        # Show the plot\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the category transition intensity analysis results (transition to target category)\n",
    "plot_transition_heatmap_v2(significant_intervals, category_transitions, selection, sub_area_dropdown.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Plot the change hotspots for two years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Choose year interval to investigate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure significant_intervals is not empty\n",
    "if 'significant_intervals' not in globals() or not significant_intervals:\n",
    "    print(\"⚠️ Warning: No significant intervals found. Ensure data is loaded before running this cell.\")\n",
    "    significant_intervals = []  # Default to an empty list if missing\n",
    "\n",
    "# Define year options for dropdown, adding a placeholder option\n",
    "year_options = significant_intervals + ['Choose here...']\n",
    "\n",
    "# Create a dropdown widget for selecting the time interval of interest\n",
    "pre_post_year_dropdown = widgets.Dropdown(\n",
    "    options=year_options,\n",
    "    value='Choose here...',  # Default selection\n",
    "    description='Year Interval:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# Define an update function for dropdown selection\n",
    "def on_pre_post_year_change(change):\n",
    "    \"\"\"\n",
    "    Updates the selected year interval when the dropdown value changes.\n",
    "\n",
    "    Parameters:\n",
    "    - change (dict): Contains old and new values for the widget.\n",
    "    \"\"\"\n",
    "    print(f\"Pre- and Post-year of investigation updated to: {change['new']}\")\n",
    "\n",
    "# Attach the update function to the dropdown\n",
    "pre_post_year_dropdown.observe(on_pre_post_year_change, names='value')\n",
    "\n",
    "# Display the dropdown in the Jupyter notebook\n",
    "display(pre_post_year_dropdown)\n",
    "\n",
    "# Display usage note for the multiple-selection tool\n",
    "print(\n",
    "    \"💡 TIP: To effectively use the multi-select tool below, hold Ctrl (Windows/Linux)/Cmd (Mac) while selecting multiple options.\"\n",
    ")\n",
    "\n",
    "# Ensure transition_label_map is available before creating the multi-select widget\n",
    "if 'transition_label_map' not in globals():\n",
    "    raise ValueError(\"Error: 'transition_label_map' is not defined. Please ensure it is loaded before running this cell.\")\n",
    "\n",
    "# Create a multi-select widget for choosing active LULCC changes\n",
    "select_widget = widgets.SelectMultiple(\n",
    "    options=list(transition_label_map.values()),  # Display transition labels\n",
    "    value=[],  # Default value (no selection)\n",
    "    description='Active Changes:',\n",
    "    disabled=False,\n",
    "    layout=Layout(width='350px', height='300px')  # Custom size\n",
    ")\n",
    "\n",
    "# Display the multi-select widget\n",
    "display(select_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Set variables based on selections above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the selected year from the dropdown\n",
    "selected_pre_post_year = pre_post_year_dropdown.value  # Fetch the currently selected year interval\n",
    "\n",
    "# Ensure the selected year is valid before modifying it\n",
    "if selected_pre_post_year and selected_pre_post_year != \"Choose here...\":\n",
    "    inv_year_range = str(selected_pre_post_year).replace('_to_', '-')  # Format year range for display\n",
    "else:\n",
    "    inv_year_range = \"Not Selected\"\n",
    "\n",
    "# Ensure transition_label_map exists before proceeding\n",
    "if 'transition_label_map' not in globals():\n",
    "    raise ValueError(\"Error: 'transition_label_map' is not defined. Please ensure it is loaded before running this cell.\")\n",
    "\n",
    "# Retrieve selected labels from the widget\n",
    "selected_labels = select_widget.value  # This returns a tuple of selected label strings\n",
    "\n",
    "# Map selected labels back to their corresponding keys\n",
    "selected_keys = [key for key, value in transition_label_map.items() if value in selected_labels]\n",
    "\n",
    "# Convert selected keys to integers, handling the case where no selections are made\n",
    "try:\n",
    "    transitions_of_interest = list(map(int, selected_keys)) if selected_keys else []\n",
    "except ValueError as e:\n",
    "    print(f\"⚠️ Warning: Unable to convert selected keys to integers: {e}\")\n",
    "    transitions_of_interest = []\n",
    "\n",
    "# Debugging Output\n",
    "print(f\"📅 Selected Year Range: {inv_year_range}\")\n",
    "print(f\"🔄 Selected Transitions (Keys): {transitions_of_interest}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Show the spatial extent of these changes on the map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_filtered_lulcc_map(results_per_area_and_year_pairs, sub_area_dropdown, pre_post_year_dropdown, transitions_of_interest):\n",
    "    \"\"\"\n",
    "    Generates an interactive map visualizing filtered LULCC transitions.\n",
    "\n",
    "    Parameters:\n",
    "    - results_per_area_and_year_pairs (dict): Dictionary containing area and year-specific results.\n",
    "    - sub_area_dropdown (ipywidgets.Dropdown): Widget for selecting the sub-area.\n",
    "    - pre_post_year_dropdown (ipywidgets.Dropdown): Widget for selecting the time interval.\n",
    "    - transitions_of_interest (list): List of selected transition values.\n",
    "\n",
    "    Returns:\n",
    "    - Displays an interactive geemap.Map with the filtered transitions.\n",
    "    \"\"\"\n",
    "    # Retrieve selected values\n",
    "    selected_sub_area = sub_area_dropdown.value\n",
    "    selected_year_range = pre_post_year_dropdown.value\n",
    "\n",
    "    # Validate selection before proceeding\n",
    "    if selected_year_range == \"Choose here...\":\n",
    "        print(\"⚠️ Error: No year range selected. Please choose a valid year range.\")\n",
    "        return\n",
    "    \n",
    "    if selected_sub_area not in results_per_area_and_year_pairs:\n",
    "        print(f\"⚠️ Error: No data found for sub-area '{selected_sub_area}'. Please select a valid area.\")\n",
    "        return\n",
    "\n",
    "    if selected_year_range not in results_per_area_and_year_pairs[selected_sub_area]:\n",
    "        print(f\"⚠️ Error: No data found for year range '{selected_year_range}' in sub-area '{selected_sub_area}'.\")\n",
    "        return\n",
    "    \n",
    "    if not transitions_of_interest:\n",
    "        print(\"⚠️ Warning: No transitions selected. The map may be empty.\")\n",
    "    \n",
    "    # Define 'combined' image based on selected sub-area and year\n",
    "    combined = results_per_area_and_year_pairs[selected_sub_area][selected_year_range]\n",
    "\n",
    "    # Start with a condition that's always False\n",
    "    mask = ee.Image(0)\n",
    "\n",
    "    # Dynamically update the mask based on selected transitions\n",
    "    for transition in transitions_of_interest:\n",
    "        mask = mask.Or(combined.eq(transition))\n",
    "\n",
    "    # Apply the mask to filter selected transitions\n",
    "    filtered_transitions = combined.updateMask(mask)\n",
    "\n",
    "    # Define visualization parameters\n",
    "    vis_params = {\n",
    "        'min': 0,\n",
    "        'max': max(transitions_of_interest) if transitions_of_interest else 1,  # Avoid max() error\n",
    "        'palette': ['red'] * len(transitions_of_interest) if transitions_of_interest else ['red']\n",
    "    }\n",
    "\n",
    "    # Initialize the geemap Map\n",
    "    Map = geemap.Map(basemap='Esri.WorldImagery')\n",
    "\n",
    "    # Add the filtered transitions layer to the map\n",
    "    Map.addLayer(filtered_transitions, vis_params, 'Filtered Transitions')\n",
    "\n",
    "    # Display the map\n",
    "    display(Map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the LULC changes spatially on the map\n",
    "generate_filtered_lulcc_map(results_per_area_and_year_pairs, sub_area_dropdown, pre_post_year_dropdown, transitions_of_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################END OF CODE#################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gee",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
